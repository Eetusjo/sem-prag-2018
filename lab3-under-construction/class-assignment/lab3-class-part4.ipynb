{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantics and Pragmatics, KIK-LG103\n",
    "\n",
    "## Lab session 3, Part 4\n",
    "\n",
    "---\n",
    "\n",
    "### Section 3.1: Sentence embeddings\n",
    "\n",
    "In this part we will take a quick look at phrase/sentence embeddings. In the last exercise in Part 3 you figured out how to average vectors. As is turns out, averaging the embeddings of the words in a phrase is also one of the simplest ways of creating an embedding for the phrase. \n",
    "\n",
    "For example, let's imagine we have four words with the following embeddings:\n",
    "\n",
    "    the       = [1, 1, 2, 1]\n",
    "    cat       = [2, 0, 1, 3]\n",
    "    is        = [1, 0, 2, 0]\n",
    "    beautiful = [4, 1, 0, 0]\n",
    "    \n",
    "The embedding for the phrase *the cat is beautiful* could be calculated by averaging the four vectors:\n",
    "\n",
    "    (the + cat + is + beautiful)/4 = [2, 0.5, 0.75, 1]\n",
    "    \n",
    "There are a multitude of ways to improve on this simple baseline, but given good word embeddings trained on a very large corpus, this method words surprisingly well on many different tasks. \n",
    "\n",
    "In the code cell below you can try out visualizing embeddings for different sentences. The intuition that guided our thinking with word vectors works here too: Similar sentences should have similar embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../src\")\n",
    "import plot_utils\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "embeddings, mapping = plot_utils.get_embeddings()\n",
    "\n",
    "sents = [\"the man saw the queen\",\n",
    "         \"the king kissed the queen\",\n",
    "         \"the boat was really fast\",\n",
    "         \"the dog ran very fast\",\n",
    "         \"the princess was trying to hug the prince\",\n",
    "         \"the dog chased the cat\",\n",
    "         \"the royal family moved to a new palace\",\n",
    "         \"the animals were running around\",\n",
    "         \"no bear has ever been seen in finland\"]\n",
    "\n",
    "plot_utils.plot_sentences_2d(sents, embeddings, mapping)\n",
    "plot_utils.plot_sentences_3d(sents, embeddings, mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex 3.1.1 TODO** Improve on baseline embedding function. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
