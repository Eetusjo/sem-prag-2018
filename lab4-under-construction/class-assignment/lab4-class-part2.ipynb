{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantics and Pragmatics, KIK-LG103\n",
    "\n",
    "## Lab session 4, Part 2\n",
    "\n",
    "---\n",
    "\n",
    "We will now move on to the second main topic of the day, **clustering**. We will try out two different clustering methods that were introduced in the lecture. The first one is **k-means** and the second one is **hierarchical (agglomerative)** clustering.\n",
    "\n",
    "Once again, import all the necessary stuff before you read on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../src/\")\n",
    "\n",
    "from lab4utils import embed, to_feature_matrix, plot_dendrogram, plot_kmeans\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Section 2.1\n",
    "\n",
    "---\n",
    "\n",
    "The first method we wil look at is a \"flat\" clustering algorithm called k-means. Flat in this case means that the resulting clusters do not have any explicit structure. The optimal result is simply that words within a cluster are maximally similar to each other, while words in different clusters are maximally different from each other. We saw a [demo](http://shabal.in/visuals/kmeans/1.html) of how k-means clustering works; if you need a refresher you can check that out again.\n",
    "\n",
    "---\n",
    "\n",
    "**Ex 2.1.1** In the code cell below we show you how to cluster a set of words and plot the results. The things you need to worry about are on the second and the third line. \n",
    "\n",
    "Try out different words and numbers of clusters and think about the following questions: \n",
    "\n",
    "- How well does the clustering work?\n",
    "- Which words seems to work best?\n",
    "- What kind of categories do you think the clusters represent?\n",
    "- Is there a number of clusters that gives sensible results most of the time, or one that doesn't work at all?\n",
    "- Do you see any problems with having to define the number of clusters yourself?\n",
    "- Do the clusters change when you run the algorithm several times?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the words to be clustered and plotted\n",
    "words = \"run jump swim walk go take cry laugh speak talk hear\".split()\n",
    "clusters = 2\n",
    "\n",
    "# Represent the words in a suitable way for the clustering algorithm\n",
    "X = to_feature_matrix(words)\n",
    "# Initialize clustering algorithm\n",
    "model = KMeans(n_clusters=clusters)\n",
    "# Train model\n",
    "model = model.fit(X)\n",
    "    \n",
    "# Plot results\n",
    "plot_kmeans(model, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Section 2.2\n",
    "\n",
    "---\n",
    "\n",
    "in this section we will look at the second method: **hierarchical agglomerative clustering**. The method is hierarchical because it gives us a hierarchy of clusters instead of the flat, structureless clusters of k-means. We can investigate the hierarchy at different levels, resulting in different clusters depending on the level where we decide to group the words. Agglomerative means that the algorithm works in a bottom-up manner. Initially, each word is considered its own cluster. The clusters are then iteratively merged until we end up with one cluster. This results in the hierarchical structure.\n",
    "\n",
    "All of this is easier to see in a dendrogram. Run the code cell below to see the results.\n",
    "\n",
    "---\n",
    "\n",
    "**Ex 2.2.1** Again, try out different words. This time the number of clusters isn't the most important thing. As you might notice, the number you define doesn't change the structure of the resulting hierachy. What it changes is the \"depth\" where the algorithm groups the words into clusters. These clusters are shown after the word labels (`word/cluster_id`). Think about the following questions:\n",
    "\n",
    "- Do you see any potential benefits to using a hierarchical clustering instead of a flat one like k-means? Any problems? \n",
    "- Do the resulting clusters from this method match on to the clusters produces by k-means?\n",
    "- Do the resulting clusters change when you run the algorithm multiple times?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the words to be clustered and plotted\n",
    "words = \"run jump swim walk go take cry laugh speak talk hear\".split()\n",
    "clusters = 4\n",
    "\n",
    "# Represent the words in a suitable way for the clustering algorithm\n",
    "X = to_feature_matrix(words)\n",
    "# Initialize clustering algorithm\n",
    "model = AgglomerativeClustering(n_clusters=4)\n",
    "model = model.fit(X)\n",
    "\n",
    "# Train model\n",
    "model = model.fit(X)\n",
    "    \n",
    "# Plot results\n",
    "plot_dendrogram(model, labels=words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
